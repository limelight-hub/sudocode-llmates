{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ae63d1",
   "metadata": {},
   "source": [
    "# **Week 1: LLM APIs 101**\n",
    "---\n",
    "üìù ***Exercises***\n",
    "\n",
    "* Call OpenAI API v√† in ra response.\n",
    "* Call Claude API v√† so s√°nh format input/output.\n",
    "* Call Gemini API v·ªõi input text (n·∫øu c√≥ key)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5c833",
   "metadata": {},
   "source": [
    "# **Basic knowledge about LLM API**\n",
    "---\n",
    "**LLM API**: stands as a technical interaction with AI systems capable of processing, comprehending, and generating human language.\n",
    "> These APIs act as a channel between the algorithms of LLM performance and various applications, enabling seamless integration of language processing functionalities into software solutions.\n",
    "\n",
    "* **Authentication**: normally request **API Key** for verification\n",
    "* **Endpoint**: Each provider (LLM Models such as OpenAI, Anthropic..) includes many different endpoint (chat completions, embeddings, images, audio...)\n",
    "* **Request Body (Parameters)**: JSON contains input information\n",
    "* **Response**: Return JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2130bd5",
   "metadata": {},
   "source": [
    "# **Parameters**\n",
    "---\n",
    "* **temparature** (0 ‚Üí 1+): s·ª± s√°ng t·∫°o trong c√¢u tr·∫£ l·ªùi c·ªßa m√¥ h√¨nh.\n",
    "    * 0 ‚Üí r·∫•t l·∫∑p l·∫°i/ƒëi th·∫≥ng, 0.7 ‚Üí s√°ng t·∫°o, 1.2 ‚Üí r·∫•t ph√≥ng kho√°ng.\n",
    "    * V√≠ d·ª•: temperature = 0 - c√¢u tr·∫£ l·ªùi chu·∫©n, temparatur = 0.8 ƒë·ªÉ vi·∫øt s√°ng t·∫°o nh∆∞ th∆°\n",
    "\n",
    "* **top_p** (0-1): nuclues sampling - ki·ªÉm so√°t s·ª± ng·∫´u nhi√™n. Thay v√¨ xem x√©t t·∫•t c·∫£ c√°c t·ª´, m√¥ h√¨nh s·∫Ω ch·ªâ xem x√©t m·ªôt t·∫≠p h·ª£p c√°c t·ª´ c√≥ x√°c su·∫•t c·ªông d·ªìn l·ªõn h∆°n gi√° tr·ªã top_p\n",
    "    * top_p=0.1: Ch·ªâ xem x√©t nh·ªØng t·ª´ c√≥ x√°c su·∫•t cao, c·ªông l·∫°i ƒë·ªß 10%. K·∫øt qu·∫£ r·∫•t t·∫≠p trung v√† √≠t ng·∫´u nhi√™n.\n",
    "    * top_p=0.9: M√¥ h√¨nh s·∫Ω xem x√©t m·ªôt t·∫≠p h·ª£p t·ª´ r·ªông h∆°n nhi·ªÅu.\n",
    "* **max-tokens/max_output_tokens**: gi·ªõi h·∫°n ƒë·ªô d√†i c·ªßa response -> > D√πng ƒë·ªÉ ki·ªÉm so√°t chi ph√≠ + ƒë·∫£m b·∫£o response ng·∫Øn g·ªçn, ƒëi ƒë√∫ng tr·ªçng t√¢m.\n",
    "* **n**: s·ªë l∆∞·ª£ng c√¢u tr·∫£ l·ªùi\n",
    "* **stream=True**: streaming response - Server s·∫Ω g·ª≠i d·∫ßn d·∫ßn t·ª´ng chunk c·ªßa c√¢u tr·∫£ l·ªùi ngay khi model sinh ra (t∆∞∆°ng t·ª± nh∆∞ xem ng∆∞·ªùi kh√°c g√µ tr·ª±c ti·∫øp tr√™n m√†n h√¨nh)\n",
    "    * stream=True th√¨ API tr·∫£ d·ªØ li·ªáu theo ki·ªÉu event stream (m·ªói d√≤ng l√† m·ªôt chunk JSON)\n",
    "    * M·ªói chunk ch·ª©a ph·∫ßn nh·ªè c·ªßa c√¢u tr·∫£ l·ªùi (delta, context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13de6fc",
   "metadata": {},
   "source": [
    "# **OpenAI**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d912776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a59d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key='API_KEYS' \n",
    ")\n",
    "\n",
    "response = client.responses.create(\n",
    "  model='gpt-4o-mini',\n",
    "  input='write a haiku about ai',\n",
    "  store=True,\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c0117",
   "metadata": {},
   "source": [
    "# **Claude - Anthropic**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9fb30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting anthropic\n",
      "  Downloading anthropic-0.67.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from anthropic) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from anthropic) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from anthropic) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.25.0->anthropic) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\pc\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
      "Downloading anthropic-0.67.0-py3-none-any.whl (317 kB)\n",
      "Installing collected packages: anthropic\n",
      "Successfully installed anthropic-0.67.0\n"
     ]
    }
   ],
   "source": [
    "# installation\n",
    "! pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00114ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=\"my_api_key\",\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-opus-4-1-20250805\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed521d",
   "metadata": {},
   "source": [
    "# **Gemini**\n",
    "---\n",
    "Need to set up the environment variable **GEMINI_API_KEY**\n",
    "* **SECURE**: Specify the variable name as GEMINI_API_KEY in \"Environment Variables\"\n",
    "* **PUBLIC**: client = genai.Client(api_key=\"YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d501cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "! pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0e93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf250486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring parameters in the Gemini model (or any large language model) allows you to control its behavior, output style, and safety settings. The exact method depends on how you're interacting with the Gemini API ‚Äì whether through Google AI Studio, the client SDKs (Python, Node.js, etc.), or the REST API.\n",
      "\n",
      "Here's a breakdown of common parameters and how to configure them:\n",
      "\n",
      "## Common Parameters You Can Configure\n",
      "\n",
      "Most LLMs, including Gemini, offer similar core parameters:\n",
      "\n",
      "1.  **`temperature`**:\n",
      "    *   **Range:** Typically 0.0 to 1.0 (though some interfaces might allow slightly higher).\n",
      "    *   **Effect:** Controls the randomness of the output.\n",
      "        *   **Lower values (e.g., 0.0 - 0.3):** Make the model more deterministic and focused, useful for tasks requiring factual, precise, or consistent answers.\n",
      "        *   **Higher values (e.g., 0.7 - 1.0):** Make the model more creative, diverse, and prone to generating unexpected or surprising outputs, good for brainstorming or creative writing.\n",
      "        *   *Note: A temperature of 0.0 does not guarantee the exact same output every time, as other factors and internal model states can still lead to minor variations.*\n",
      "\n",
      "2.  **`top_p` (nucleus sampling)**:\n",
      "    *   **Range:** Typically 0.0 to 1.0.\n",
      "    *   **Effect:** The model considers the smallest set of tokens whose cumulative probability exceeds `top_p`. It then samples a token from this set.\n",
      "        *   **Lower values:** Narrows the token choices to only the most probable ones.\n",
      "        *   **Higher values:** Allows for a wider range of tokens, increasing diversity.\n",
      "    *   *Often used in conjunction with `top_k`.*\n",
      "\n",
      "3.  **`top_k` (top-k sampling)**:\n",
      "    *   **Range:** Typically a positive integer (e.g., 1-40).\n",
      "    *   **Effect:** The model considers the `top_k` most probable tokens at each step and then samples from that reduced set.\n",
      "        *   **Lower values:** Restricts choices to the very top candidates, leading to more predictable output.\n",
      "        *   **Higher values:** Broadens choices, increasing diversity.\n",
      "\n",
      "4.  **`max_output_tokens` / `max_tokens`**:\n",
      "    *   **Range:** Typically an integer (e.g., 1 to 2048, depending on the model and API limits).\n",
      "    *   **Effect:** Sets the maximum number of tokens the model will generate in its response. Useful for controlling response length and cost.\n",
      "\n",
      "5.  **`stop_sequences`**:\n",
      "    *   **Effect:** A list of strings. If the model generates any of these strings, it will stop generating further output immediately. Useful for ensuring the model doesn't go off-topic or generates unwanted continuations.\n",
      "    *   *Example:* `[\"\\n\\n\", \"User:\"]` might stop the model if it tries to start a new turn or conversational segment.\n",
      "\n",
      "6.  **`candidate_count`**:\n",
      "    *   **Effect:** The number of alternative responses the model should generate. You can then pick the best one.\n",
      "    *   *Note: Generating multiple candidates consumes more tokens and can increase latency and cost.*\n",
      "\n",
      "7.  **`safety_settings`**:\n",
      "    *   **Effect:** Allows you to set thresholds for different content categories (e.g., Harassment, Hate Speech, Sexually Explicit, Dangerous Content). If the generated content crosses these thresholds, the response will be blocked or partially filtered.\n",
      "    *   You typically specify a category and a threshold (e.g., `HARM_CATEGORY_HARASSMENT: HARM_BLOCK_THRESHOLD_MEDIUM`).\n",
      "\n",
      "## How to Configure Parameters\n",
      "\n",
      "### 1. Using Google AI Studio (Web UI)\n",
      "\n",
      "This is the easiest way to experiment with parameters.\n",
      "\n",
      "1.  Go to [Google AI Studio](https://aistudio.google.com/).\n",
      "2.  Start a new \"Freeform prompt\" or \"Chat prompt.\"\n",
      "3.  On the right-hand sidebar, you'll see a **\"Parameters\"** section.\n",
      "4.  You can adjust `Temperature`, `Top K`, `Top P`, `Max output tokens`, and add `Stop sequences` using sliders and input fields.\n",
      "5.  There's also a **\"Safety settings\"** section where you can adjust the blocking thresholds for different content categories.\n",
      "6.  Click \"Run\" or \"Send\" to see the effect of your changes.\n",
      "\n",
      "### 2. Using the Python SDK\n",
      "\n",
      "This is the most common programmatic way to interact with Gemini.\n",
      "\n",
      "```python\n",
      "import google.generativeai as genai\n",
      "import os\n",
      "\n",
      "# Configure your API key\n",
      "# It's recommended to store your API key securely, e.g., in an environment variable\n",
      "# genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
      "# OR directly:\n",
      "genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
      "\n",
      "# Choose a model\n",
      "model = genai.GenerativeModel('gemini-pro')\n",
      "\n",
      "# Define generation configuration\n",
      "# These are the parameters that control the model's output style\n",
      "generation_config = {\n",
      "    \"temperature\": 0.8,         # More creative/random\n",
      "    \"top_p\": 0.95,              # Samples from a wider set of high-probability tokens\n",
      "    \"top_k\": 40,                # Considers top 40 tokens\n",
      "    \"max_output_tokens\": 500,   # Max length of the response\n",
      "    \"stop_sequences\": [\"###\"],  # Stop if \"###\" is generated\n",
      "    \"candidate_count\": 1        # Generate 1 response candidate\n",
      "}\n",
      "\n",
      "# Define safety settings\n",
      "# These are the parameters that control content moderation\n",
      "# You typically set a threshold for each harm category\n",
      "safety_settings = [\n",
      "    {\n",
      "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
      "    },\n",
      "    {\n",
      "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
      "    },\n",
      "    {\n",
      "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
      "    },\n",
      "    {\n",
      "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
      "    },\n",
      "]\n",
      "\n",
      "# Make a generation request with the configured parameters\n",
      "prompt = \"Write a short, imaginative story about a teapot who dreams of flying.\"\n",
      "\n",
      "try:\n",
      "    response = model.generate_content(\n",
      "        prompt,\n",
      "        generation_config=generation_config,\n",
      "        safety_settings=safety_settings\n",
      "    )\n",
      "\n",
      "    print(\"Generated Text:\")\n",
      "    print(response.text)\n",
      "\n",
      "    # Access other candidates if candidate_count > 1\n",
      "    # for i, candidate in enumerate(response.candidates):\n",
      "    #     print(f\"\\nCandidate {i+1}:\")\n",
      "    #     print(candidate.text)\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "    if hasattr(e, 'response') and hasattr(e.response, 'prompt_feedback'):\n",
      "        print(f\"Prompt Feedback: {e.response.prompt_feedback}\")\n",
      "        # If content was blocked due to safety settings:\n",
      "        for rating in e.response.prompt_feedback.safety_ratings:\n",
      "            print(f\"  Category: {rating.category}, Probability: {rating.probability}, Blocked: {rating.blocked}\")\n",
      "\n",
      "```\n",
      "\n",
      "### 3. Using Other SDKs (Node.js, Go, Java, etc.)\n",
      "\n",
      "The pattern is very similar to Python:\n",
      "\n",
      "1.  Import the relevant Gemini library.\n",
      "2.  Initialize the client with your API key.\n",
      "3.  Define a configuration object (often named `generationConfig` or similar) with your desired parameters (e.g., `temperature`, `maxOutputTokens`).\n",
      "4.  Define a `safetySettings` array/list of objects.\n",
      "5.  Pass these configuration objects to your `generateContent` (or equivalent) method.\n",
      "\n",
      "**Example (Node.js - conceptual):**\n",
      "\n",
      "```javascript\n",
      "// const { GoogleGenerativeAI } = require(\"@google/generative-ai\");\n",
      "// const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);\n",
      "\n",
      "// const model = genAI.getGenerativeModel({ model: \"gemini-pro\" });\n",
      "\n",
      "// const generationConfig = {\n",
      "//   temperature: 0.8,\n",
      "//   topP: 0.95,\n",
      "//   topK: 40,\n",
      "//   maxOutputTokens: 500,\n",
      "//   stopSequences: [\"###\"],\n",
      "//   candidateCount: 1,\n",
      "// };\n",
      "\n",
      "// const safetySettings = [\n",
      "//   { category: \"HARM_CATEGORY_HARASSMENT\", threshold: \"BLOCK_MEDIUM_AND_ABOVE\" },\n",
      "//   // ... other safety settings\n",
      "// ];\n",
      "\n",
      "// async function run() {\n",
      "//   const result = await model.generateContent({\n",
      "//     contents: [{ role: \"user\", parts: [{ text: \"Write a short, imaginative story...\" }] }],\n",
      "//     generationConfig,\n",
      "//     safetySettings,\n",
      "//   });\n",
      "//   const response = result.response;\n",
      "//   console.log(response.text());\n",
      "// }\n",
      "\n",
      "// run();\n",
      "```\n",
      "\n",
      "### 4. Using the REST API\n",
      "\n",
      "If you're making direct HTTP requests, you'll include these parameters in the JSON request body.\n",
      "\n",
      "```http\n",
      "POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=YOUR_API_KEY\n",
      "Content-Type: application/json\n",
      "\n",
      "{\n",
      "  \"contents\": [\n",
      "    {\n",
      "      \"parts\": [\n",
      "        {\n",
      "          \"text\": \"Write a short, imaginative story about a teapot who dreams of flying.\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"generationConfig\": {\n",
      "    \"temperature\": 0.8,\n",
      "    \"topP\": 0.95,\n",
      "    \"topK\": 40,\n",
      "    \"maxOutputTokens\": 500,\n",
      "    \"stopSequences\": [\"###\"],\n",
      "    \"candidateCount\": 1\n",
      "  },\n",
      "  \"safetySettings\": [\n",
      "    {\n",
      "      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "### 5. Using Vertex AI (for Enterprise/Production)\n",
      "\n",
      "If you're using Gemini through Google Cloud's Vertex AI platform, you'll typically use the `google.cloud.aiplatform` client library. The parameters are conceptually the same, but the exact client library methods and object structures might differ slightly.\n",
      "\n",
      "You'd generally import `aiplatform` and then interact with a `PredictionServiceClient` to send your prompt with `generation_config` and `safety_settings`.\n",
      "\n",
      "```python\n",
      "# from google.cloud import aiplatform\n",
      "\n",
      "# # Initialize Vertex AI\n",
      "# aiplatform.init(project=\"your-gcp-project-id\", location=\"us-central1\")\n",
      "\n",
      "# # Load a Gemini model deployed on Vertex AI\n",
      "# model = aiplatform.GenerativeModel(model_name=\"gemini-pro\") # Or a specific model ID\n",
      "\n",
      "# # Same generation_config and safety_settings as above\n",
      "# generation_config = { ... }\n",
      "# safety_settings = [ ... ]\n",
      "\n",
      "# response = model.generate_content(\n",
      "#     \"Your prompt here\",\n",
      "#     generation_config=generation_config,\n",
      "#     safety_settings=safety_settings\n",
      "# )\n",
      "\n",
      "# print(response.text)\n",
      "```\n",
      "\n",
      "## Best Practices\n",
      "\n",
      "*   **Experiment:** The best way to understand how parameters affect output is to experiment with different values for your specific use case.\n",
      "*   **Balance Creativity vs. Control:** High `temperature` or `top_p`/`top_k` values increase creativity but can also lead to less predictable or hallucinated content. Low values increase control and predictability but might make the output feel generic.\n",
      "*   **Cost and Latency:** Generating multiple candidates (`candidate_count`) or very long responses (`max_output_tokens`) will increase the token usage, cost, and response time.\n",
      "*   **Safety First:** Always consider and appropriately configure `safety_settings` to ensure your application generates responsible and harmless content.\n",
      "*   **Context is Key:** Remember that parameters complement your prompt. A well-crafted prompt will always be the most impactful way to guide the model.\n",
      "\n",
      "By understanding and effectively using these parameters, you can fine-tune Gemini's behavior to meet the specific requirements of your application.\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "\n",
    "client = genai.Client() # secret way :>\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"how can i configure parameters in gemini model\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f69b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a professional Data Scientist, when I explain **LSTM (Long Short-Term Memory)**, I typically start by situating it within the broader context of neural networks designed for sequential data.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "---\n",
      "\n",
      "### What is LSTM?\n",
      "\n",
      "**LSTM (Long Short-Term Memory)** is a specialized type of **Recurrent Neural Network (RNN)** architecture specifically designed to overcome the limitations of traditional RNNs in learning long-term dependencies. In simpler terms, it's a neural network that is very good at remembering things for a long time, which is crucial when dealing with sequences of data like text, speech, or time series.\n",
      "\n",
      "---\n",
      "\n",
      "### The Problem LSTMs Solve: Long-Term Dependencies & Vanishing Gradients\n",
      "\n",
      "Traditional RNNs suffer from two main issues when processing long sequences:\n",
      "\n",
      "1.  **Vanishing Gradient Problem:** During backpropagation (the process of updating network weights), gradients (signals that indicate how much to change weights) can shrink exponentially as they propagate back through many layers. This makes it very difficult for the network to learn and remember information from earlier steps in a long sequence. It effectively \"forgets\" what happened a long time ago.\n",
      "2.  **Exploding Gradient Problem:** Less common, but gradients can also grow exponentially, leading to unstable training.\n",
      "\n",
      "This means standard RNNs struggle with **long-term dependencies** ‚Äì the ability to connect information that appeared much earlier in the sequence to the current prediction. For example, in a sentence like \"The man who lives in the large, blue house with a red roof, a big garden, and a friendly dog is very happy,\" to correctly predict \"happy,\" the model needs to remember \"The man,\" not just the most recent words.\n",
      "\n",
      "---\n",
      "\n",
      "### How LSTMs Work: The \"Memory Cell\" and \"Gates\"\n",
      "\n",
      "LSTMs address these problems by introducing a sophisticated internal structure called a **memory cell (or cell state)** and several **gates** that regulate the flow of information into and out of this cell.\n",
      "\n",
      "Imagine the memory cell as a kind of conveyor belt that runs through the entire LSTM chain, carrying information (the \"state\") forward. The gates then decide how much new information gets added to the belt, how much old information gets forgotten, and what information is relevant to output at each step.\n",
      "\n",
      "Here are the key components within an LSTM unit:\n",
      "\n",
      "1.  **Cell State (C_t):**\n",
      "    *   This is the core \"memory\" of the LSTM. It runs straight through the entire chain of units, largely undisturbed.\n",
      "    *   Information can be added to or removed from the cell state, but it's the pathway that allows information to persist over long sequences.\n",
      "\n",
      "2.  **Gates:** These are special neural networks (typically using sigmoid activation functions) that output values between 0 and 1, acting as \"filters\" or \"switches\" to control the information flow.\n",
      "\n",
      "    *   **a. Forget Gate (f_t):**\n",
      "        *   **Purpose:** Decides what information from the previous cell state (`C_{t-1}`) should be thrown away or forgotten.\n",
      "        *   **Mechanism:** It looks at the previous hidden state (`h_{t-1}`) and the current input (`x_t`), and outputs a number between 0 and 1 for each element in the cell state. A 0 means \"forget this completely,\" and a 1 means \"keep this completely.\"\n",
      "        *   **Intuition:** \"Is this old information still relevant for our current task?\" (e.g., if we encounter a new subject in a sentence, we might forget the old one's gender).\n",
      "\n",
      "    *   **b. Input Gate (i_t) & Candidate Cell State (C_tilde_t):**\n",
      "        *   **Purpose:** Decides what new information from the current input (`x_t`) should be stored in the cell state.\n",
      "        *   **Mechanism (Input Gate):** A sigmoid layer decides which values to update.\n",
      "        *   **Mechanism (Candidate Cell State):** A `tanh` layer creates a vector of new candidate values (`C_tilde_t`) that *could* be added to the cell state.\n",
      "        *   **Combined:** The input gate then multiplies its output by the candidate cell state (`i_t * C_tilde_t`) to decide how much of the new information to add.\n",
      "        *   **Intuition:** \"What new information just arrived, and how much of it is important enough to add to our memory?\"\n",
      "\n",
      "    *   **c. Update the Cell State:**\n",
      "        *   This is where the forget and input decisions come together.\n",
      "        *   The old cell state is multiplied by the forget gate (`f_t * C_{t-1}`), dropping the information we decided to forget.\n",
      "        *   Then, the output of the input gate is added (`i_t * C_tilde_t`), incorporating the new relevant information.\n",
      "        *   **Result:** A new cell state (`C_t`) is formed.\n",
      "\n",
      "    *   **d. Output Gate (o_t):**\n",
      "        *   **Purpose:** Decides what part of the current cell state (`C_t`) should be outputted as the hidden state (`h_t`).\n",
      "        *   **Mechanism:** A sigmoid layer decides which parts of the cell state are relevant for the current output.\n",
      "        *   **Combined:** The cell state then goes through a `tanh` function (to push values between -1 and 1), and is multiplied by the output of the sigmoid gate (`o_t * tanh(C_t)`). This gives us the new hidden state (`h_t`).\n",
      "        *   **Intuition:** \"Based on our current memory, what's the most relevant information to show *right now* for the next step or final prediction?\"\n",
      "\n",
      "---\n",
      "\n",
      "### Advantages of LSTMs\n",
      "\n",
      "*   **Handle Long-Term Dependencies:** Their primary strength, allowing them to learn and retain information over many time steps.\n",
      "*   **Mitigate Vanishing Gradients:** The explicit cell state and gate mechanism provide a clear path for gradients to flow backward, preventing them from vanishing.\n",
      "*   **Effective for Sequential Data:** Excel in tasks involving time series, natural language, and other ordered data.\n",
      "\n",
      "---\n",
      "\n",
      "### Common Use Cases\n",
      "\n",
      "LSTMs have been incredibly successful in various fields, including:\n",
      "\n",
      "*   **Natural Language Processing (NLP):**\n",
      "    *   Machine Translation\n",
      "    *   Text Generation (e.g., chatbots, predictive text)\n",
      "    *   Sentiment Analysis\n",
      "    *   Named Entity Recognition\n",
      "*   **Speech Recognition:** Converting audio into text.\n",
      "*   **Time Series Forecasting:** Predicting stock prices, weather patterns, etc.\n",
      "*   **Image Captioning:** Describing the content of images (often combined with CNNs).\n",
      "*   **Video Analysis:** Activity recognition.\n",
      "\n",
      "---\n",
      "\n",
      "### Beyond LSTMs (Briefly)\n",
      "\n",
      "While LSTMs were a breakthrough and remain highly relevant, the field has evolved. Newer architectures like **GRUs (Gated Recurrent Units)** offer a simpler, two-gate alternative to LSTMs, often with comparable performance. More recently, **Transformers** have emerged as the dominant architecture for many sequence-to-sequence tasks, particularly in NLP, due to their attention mechanisms that allow parallel processing and better handling of very long sequences. However, LSTMs still find use in specific applications and provide a fundamental understanding of how to manage memory in sequential models.\n"
     ]
    }
   ],
   "source": [
    "# set a roles\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"what is lstm\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a professional Data Scientist\"  # give it a role\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad69fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe is an open-source framework developed by Google that provides **highly optimized, pre-built, and customizable machine learning (ML) pipelines for various computer vision (and other AI) tasks**. Its primary goal is to make it easier\n",
      " for developers to integrate real-time, on-device ML solutions into their applications across different platforms (Android, iOS, web, desktop).\n",
      "\n",
      "In computer vision, MediaPipe is used for a wide range of applications, primarily focusing on **understanding\n",
      " human actions and features**, but also extending to other object-related tasks.\n",
      "\n",
      "Here's a breakdown of its key uses in computer vision:\n",
      "\n",
      "1.  **Face Detection and Tracking:**\n",
      "    *   **Use:** Identifying the\n",
      " presence and location of faces in images or video streams.\n",
      "    *   **Applications:** Augmented reality (AR) filters, virtual try-ons, video conferencing (e.g., placing effects on faces), audience engagement analysis.\n",
      "\n",
      "2\n",
      ".  **Facial Landmark Detection (Face Mesh):**\n",
      "    *   **Use:** Pinpointing hundreds of 3D facial keypoints (e.g., eyes, nose, mouth, eyebrows, jawline).\n",
      "    *   **\n",
      "Applications:** Highly realistic AR masks and overlays, detailed facial expression analysis, 3D face reconstruction, virtual avatars.\n",
      "\n",
      "3.  **Hand Landmark Detection and Tracking:**\n",
      "    *   **Use:** Detecting the presence of hands and precisely\n",
      " locating 21 keypoints per hand (fingertips, knuckles, wrist).\n",
      "    *   **Applications:** Gesture control for smart devices, sign language interpretation, virtual reality (VR) interaction, medical rehabilitation, precise object manipulation in AR/VR\n",
      ".\n",
      "\n",
      "4.  **Pose Estimation (Body Landmarks):**\n",
      "    *   **Use:** Identifying the key joints and limbs of the human body (e.g., shoulders, elbows, hips, knees) in 2D or \n",
      "3D.\n",
      "    *   **Applications:** Fitness and sports analytics (form correction, rep counting), motion capture for animation, gaming (body-based controls), accessibility tools, safety monitoring (fall detection).\n",
      "\n",
      "5.  **Hol\n",
      "istic Tracking:**\n",
      "    *   **Use:** Combining face, hand, and pose tracking simultaneously for a comprehensive understanding of human movement and expression.\n",
      "    *   **Applications:** More advanced human-computer interaction, performance analysis, full-body AR\n",
      " experiences, behavioral studies.\n",
      "\n",
      "6.  **Object Detection and Tracking:**\n",
      "    *   **Use:** Identifying and localizing specific objects within a scene. While not its most specialized area compared to human-centric tasks, it provides pipelines\n",
      " for general object detection.\n",
      "    *   **Applications:** Inventory management, smart surveillance, interactive retail displays.\n",
      "\n",
      "7.  **Segmentation (e.g., Hair, Selfie, Portrait):**\n",
      "    *   **Use:** D\n",
      "ifferentiating a person or specific body parts (like hair) from the background, creating a mask.\n",
      "    *   **Applications:** Virtual backgrounds in video calls, green screen effects, creative photo/video editing, privacy-preserving blurring.\n",
      "\n",
      "8.\n",
      "  **Objectron (3D Object Detection):**\n",
      "    *   **Use:** Detecting common objects (like shoes, chairs, cups, cameras) and estimating their 3D bounding box and pose.\n",
      "    *   **Applications:**\n",
      " AR applications for product visualization, scene understanding for robotics.\n",
      "\n",
      "**Key Advantages of MediaPipe for Computer Vision:**\n",
      "\n",
      "*   **Real-time Performance:** Designed for extremely low latency, enabling interactive experiences.\n",
      "*   **On-device\n",
      " Processing:** Runs directly on the user's device, ensuring privacy, reducing latency, and eliminating reliance on cloud servers.\n",
      "*   **Cross-Platform Compatibility:** Works seamlessly across Android, iOS, web (JavaScript), and desktop (Python,\n",
      " C++).\n",
      "*   **Pre-trained Models:** Provides high-quality, pre-trained ML models, significantly reducing development time and effort.\n",
      "*   **Modularity and Customization:** Built with a \"graph\" architecture that\n",
      " allows developers to customize pipelines or combine different modules.\n",
      "*   **Ease of Use:** Offers user-friendly APIs, especially for Python, making it accessible to a wider range of developers.\n",
      "\n",
      "In essence, MediaPipe democratizes advanced computer vision capabilities\n",
      ", allowing developers to easily build sophisticated, real-time, and robust AI features into their applications without needing deep expertise in machine learning model training or optimization.\n"
     ]
    }
   ],
   "source": [
    "# streaming response\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "for chunk in client.models.generate_content_stream(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"what is mediapipe use for in computer vision?\"\n",
    "):\n",
    "    print(chunk.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
